# AWS S3

## Buckets
- Buckets are containers for **object** (analogous to files). Buckets are the top-level containers for objects in S3.
- Buckets are created in a specific AWS region.
- Bucket names must be **globally unique**. Most other names in AWS (e.g., IAM users) only need to be regionally unique.
- Buckets can hold an unlimited number of objects (infinitely scalable).

### Specifics
- 3-63 character length for bucket names. Must start with a lowercase letter or number.
- Can't be IP formatted
- **100 buckets soft limit** in an AWS account. You can exceed it with a request to AWS support.
- **1000 buckets hard limit** in an AWS account.

### Objects 
Objects are the files stored in S3.
- The **key** is the unique identifiers for each object in a S3 bucket.
- The **value** is the object's binary data. The value can range from **0B to 5TB** (*exam point*)

### Blast Radius
Bucket data stays within its region unless you allow it to leave the region.

### Bucket Structure
Buckets are flat structures; it is not a file system. All objects are stored at the same level.
- S3 does not use a folder structure, but it **visually** emulates one if `/` characters are used in the object key (e.g., `/old/koala1.png`)

### Hardware
- S3 is an object store. Not a file store or block store.
- It is not a file store (e.g., no folder structure)
- It is not a block store (virtual hard disk) and cannot be mounted as a mount point or volume. Block storage is limited to one user accessing it at a time; S3 does not have this limitation.

### Architecture Choices
S3 should be the default choice for offloading data transfer and access.


## Bucket Policies (Resource Policies)
S3 is private by default.
- S3 **bucket policies** are **resource policies** that govern access.
- Resource policies can allow/deny multiple accounts. 
- Resource policies can allow/deny access to anonymous accounts.
- Resource policies include a `Principal` property in their JSON. It is a distinguishing feature between Resource Policies and Identity Policies. Identity Policies do not need a Principal because they are applied to the relevant Principal (IAM user, IAM group, IAM role, etc.).
- In contrast, Identity Policies can only be attached to an identity in the account. They cannot allow/deny access to other accounts or anonymous accounts.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicRead",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::my-bucket/*"
        }
    ]
}

```

### Conditions
Bucket policies can be configued with conditions to further restrict access, such as the IP address of the requester. MFA can also be required.

### Access Control Lists (ACLs)
ACLs are a legacy method for managing access to S3 resources. ACLs function on objects and buckets. ACLs are not recommended because they are inflexible and only support simple policies. They do not support conditions like bucket policies.

|Permission|Bucket|Object|
|---|---|---|
|Read|Allows grantee to list the objects in the bucket|Allows grantee to read the object data and its metadata|
|Write|Allows grantee to create, overwrite, and delete objects in the bucket|Not applicable|
|Read-ACP|Allows grantee to read the bucket ACL|Allows grantee to read the object ACL|
|Write-ACP|Allows grantee to write the bucket ACL|Allows grantee to write the object ACL|
|Full Control|Allows grantee to READ, WRITE, READ_ACP, and WRITE_ACP on the bucket|Allows grantee to READ, READ_ACP, and WRITE_ACP on the object|

## Block Public Access
Block Public Access is a feature that helps prevent public access to S3 buckets; it was introduced to mitigate incorrect ACL configurations. 

Block Public Access is enabled by default for all new S3 buckets and covers the following:
- Block public access to buckets and objects granted through *new* ACLs
- Block public access to bucket and objects granted through *any* ACLs
- Block public access to bucket and objects granted through *new* bucket policies or access point policies
- Block public access to bucket and objects granted through *any* bucket policies or access point policies

## Permissions Policies
Buckets and objects can be configured via (1) identity policies, (2) resource policies, or (3) ACLs.
- Identity policies are useful for managing different resources, if there is a preference for IAM, or if the policy applies to a single account.
- Bucket policies are useful if managing only S3 or for allowing anonymous or cross-account access.
- ACLs generally should not be used.

## Static Website Hosting
- When creating a new bucket, the bucket policy must allow public read access. This setting technically *allows* you to configure public access to the bucket, but it does not make the contents public itself.
- The bucket policy, under properties -> bucket policy, must then be updated to explicitly allow public read access.
- Static website hosting must be enabled under properties -> static website hosting.

## S3 Versioning

- If versioning is enabled and an object is deleted in non-version-view mode, the object is not deleted. Instead, it is marked as deleted. Deleting the marker fully restores the object.
- If versioning is enabled and an object is deleted in version-view mode, the object is permanently deleted.

## Operations

## PUT (Upload)
S3 objects can be uploaded using either (1) single stream upload or (2) multipart upload. Multipart upload is strongly recommended to avoid failed uploads from disruptions.

- Objects must be smaller than 5GB.
- Multipart upload requires objects to be at least 100MB.
- Maximum number of parts is 10,000.
- Multipart part sizes must be between 5MB and 5GB. 

### S3 Transfer Acceleration
Uploading data to S3 can be slow when the data source is far away from the S3 bucket; public internet routing can be suboptimal. S3 Transfer Acceleration is a feature that speeds up the transfer of files over long distances by using AWS's global network of edge locations to optimize routing.

- Transfer Acceleration is disabled by default. Enabling it has two requirements:
    - Bucket name must be DNS compatible.
    - Bucket name cannot contain periods.

### Encryption
S3 objects can be encrypted client-side or server-side. For client-side encryption, the user must manage the encryption keys and encrypt the data before sending it to S3. 

For server-side encryption, the user can choose the type of encryption and S3 will handle the encryption and decryption. There are three types of server-side encryption for S3 buckets. Symmetric encryption is used for all three.
- **SSE-C**:Customer provided keys
- **SSE-S3**: Amazon S3-managed keys
- **AWS KMS-managed keys** (SSE-KMS)


In SSE-C, the customer sends the plaintext data and a key over HTTPS to AWS. Encryption and decryption are both offloaded to AWS. 
- AWS encrypts the data using the key and stores the resulting ciphertext of the data and a hash of the key. 
- The key is discarded afterwards, but the hash can be used to verify the key if it is used for decrpytion.


In SSE-S3, the customer sends the plaintext data over HTTPS to AWS. 
- AWS generates a plaintext key for each object and encrypts the data using the key. 
- AWS stores the ciphertext and the ciphertext key. The plaintext key is then discarded. 
- SSE-S3 uses AES-256 encryption and handles key rotation.
- SSE-S3 is not suitable for highly regulated industries that require fine grained control over key rotation or role separation (e.g., separating generation of keys from the encryption and decryption of data).

In SSE-KMS, the customer sends the plaintext data over HTTPS to AWS. 
- A KMS Key is used to encrypt the data. The KMS Key can either be (1) a custom KMS Key created by the user or (2) an AWS managed KMS Key.
    - AWS managed KMS Keys can NOT have their key policies or key rotation settings configured.
    - Custom KMS Keys can have their key policies and key rotation settings configured.
- AWS Services that want to encrypt an object using SSE-KMS, will request a new data encryption key (DEK) using a chosen KMS Key. 
- KMS will return a plaintext and ciphertext version of the DEK.
- The plaintext DEK will be used to encrypt the data. The encrypted data and ciphertext DEK are stored in S3. The plaintext DEK is discarded afterwards.

## Storage Classes

### S3 Standard
- Replicated across 3 availability zones (AZs)
    - 99.999999999% durability (11 9s) for 10M objects with an expected loss of 1 object per 10K years
    - MD5 hashes and Cyclic Redundancy Check (CRC) checksums are used to verify data integrity
    - S3's API responds with a **HTTP 1.1/200 OK** status code if an object is stored durably (*exam point*)
- Billing
    - Storage: A GB/month storage fee is charged for data. 
    - Transfer: A $/GB for transfer OUT and a price per 1K requests. Free for transfer IN
- Low latency and high throughput
    - **Millisecond** first byte latency and objects can be made publicly available

### S3 Standard - Infrequent Access (S3 Standard-IA)
- Same replication, durability, and availability behavior as S3 Standard
- Billing
    - Storage: Lower storage costs than S3 Standard. Approximately 1/2 the cost of S3 Standard.
    - Transfer: A $/GB for transfer OUT and a price per 1K requests. Free for transfer IN.
    - Retrieval: A per GB fee for retrieval that is in addition to the transfer fee.
    - Minimum duration charge: A minimum **30 day fee** is charged.
    - Capacity: Standard-IA has a minimum **128KB** object size fee so it should not be used for very small objects.
- Use cases: Use for long-lived data where data access is (1) infrequent and (2) not small.

### S3 One Zone-IA
- Same durability (11 9s) and availability (99.99%) as S3 Standard. 
    - **Only one AZ is used** so if the AZ fails, the data is lost.
    - S3 One-Zone-IA can **NOT** transition to S3 Glacier - Instant Retrieval. It can only transition to S3 Glacier - Flexible Retrieval and S3 Glacier - Deep Archive.
- Same baseline billing as S3 Standard-IA (storage fee,transfer fee, retrieval fee, minimum charge, and minimum capacity fee).
    - Storage fee is lower than S3 Standard-IA because the data is only stored in one AZ.
- Use cases: Use for long-lived data where data access is (1) infrequent, (2) not small, and (3) **non-critical data** (*exam point*) in case the AZ fails.


### S3 Glacier - Instant Retrieval
- Behaves like S3 Standard-IA, but offers cheaper storage while imposing a retrieval fee when data is retrieved.
- Same 3 AZ replication, durability, and availability behavior as S3 Standard
- **Same instant access and public access behavior as S3 Standard-IA.**
- Billing
    - Storage: A GB/month storage fee is charged for data. 
    - Transfer: A $/GB for transfer OUT and a price per 1K requests. Free for transfer IN
    - Retrieval: S3 Glacier does **NOT** have a retrieval process like the other S3 Glacier classes, but it **DOES** impose an additional **retrieval fee** when data is retrieved.
    - Minimum duration charge: A minimum **90 day fee** is charged.
    - Capacity: Standard-IA has a minimum **128KB** object size fee.
- Use cases: Use for archival data where real-time access is not required (e.g., yearly access).
- Use case: Use for data that needs to be instantly accessible, but only occasionally (e.g., quarterly)

### S3 Glacier - Flexible Retrieval
- Same 3 AZ replication, durability, and availability behavior as S3 Standard
- **Cold storage ("chilled state")**
    - Instant access and public access is not supported.
    - S3 Glacier stores a pointer to the data and a **retrieval process** with a **retrieval fee** that must be run to access the data. When retrieved, the data is stored in a S3 Standard-IA bucket temporarily.
- **Retrieval jobs** have a first-byte latency on the order of minutes to hours (faster is more expensive)
    - **Expedited**: 1-5 minutes
    - **Standard**: 3-5 hours
    - **Bulk**: 5-12 hours
- Billing
    - Storage: A GB/month storage fee is charged for data. 
    - Transfer: A $/GB for transfer OUT and a price per 1K requests. Free for transfer IN
    - Retrieval: A per GB fee for retrieval that is in addition to the transfer fee.
    - Minimum duration charge: A minimum **90 day fee** is charged.
    - Capacity: Standard-IA has a minimum **40KB** object size fee.
- Use cases: Use for archival data where real-time access is not required (e.g., yearly access).


### S3 Glacier - Deep Archive
- Same 3 AZ replication, durability, and availability behavior as S3 Standard
- Billing
    - Minimum duration charge: A minimum **180 day fee** is charged.
    - Capacity: Standard-IA has a minimum **40KB** object size fee.
- **Cold storage ("frozen state")**
    - Instant access and public access is not supported.
    - S3 Glacier stores a pointer to the data and a **retrieval process** with a **retrieval fee** that must be run to access the data. When retrieved, the data is stored in a S3 Standard-IA bucket temporarily.
- **Retrieval jobs** have a first-byte latency on the order of hours to days (faster is more expensive)
    - **Standard**: 12 hours
    - **Bulk**: 48 hours
- Use cases: Use for archival purposes where data is almost never accessed (e.g., audits). It should not be used for primary system backup or restoration purposes.

### S3 Intelligent-Tiering
S3 Intelligent-Tiering monitors usage patterns and automatically moves objects between tiers to optimize costs and performance.
- Contains 5 different storage tiers:
    - **Frequent Access**: Like S3 Standard
    - **Infrequent Access**: Like S3 Standard-IA
    - **Archive Instant Access**: Same as S3 Glacier - Instant Retrieval
    - **Archive Access**: Same as S3 Glacier - Flexible Retrieval
    - **Deep Archive Access**: Same as S3 Glacier - Deep Archive
- **Behavior**: Any object that is not accessed after a set period of time is moved to the next tier.
    - **30 days**: Object is moved to the **Infrequent Access** tier 
    - **90 days**: Object is moved to the **Archive Instant Access** tier
        - Note: There is no minimum duration charge for Archive Instant Access.
    - **90-270 days** (configurable optional tier): Object is moved to the **Archive Access** tier
    - **180-730 days** (configurable optional tier): Object is moved to the **Deep Archive Access** tier
- **Billing**:
    - Data is automatically moved up to the next tier if it is accessed **without a retrieval charge**.
    - A **monitoring and automation fee (management fee)** is charged per 1000 objects per month.
- Use case: Use for long-lived data with unknown access patterns. Implement the Archive Access and Deep Archive Access tiers only if the application can support them since API calls are needed to move objects between tiers.

## Lifecycle Configuration
A lifecycle is a **set of rules** that consist of **actions** on a **bucket** or **groups of objects in a bucket**.

### Types of Actions
- **Transition**: Moves objects between storage classes (e.g., S3 Standard to S3 Standard-IA after 30 days).
- **Expiration**: Deletes objects after a specified period of time (e.g., delete objects or object versionsafter 100 days).


### Lifecycle Transition Rules
Objects can be transitioned to a colder storage class roughly following a waterfall model. Each class can transition to **any** class below it. The only exception is S3 One Zone-IA, which can **NOT** transition to S3 Glacier - Instant Retrieval. It can only transition to S3 Glacier - Flexible Retrieval and S3 Glacier - Deep Archive.

```
- S3 Standard
  - S3 Standard-IA
    - S3 Intelligent-Tiering
      - S3 One Zone-IA
        - S3 Glacier - Instant Retrieval
          - S3 Glacier - Flexible Retrieval
            - S3 Glacier - Deep Archive
```

#### S3 Standard Object Special Rules
- S3 Standard objects must remain there for at least 30 days before they can transition to S3 Standard-IA or S3 One Zone-IA (*exam point*). 
    - Objects can be loaded directly into S3 Standard-IA or S3 One Zone-IA, but must wait if they started in S3 Standard.
    - A single rule cannot be configured to automatically transition an S3 Standard object first to S3 Standard-IA or S3 One Zone-IA and then to a glacier class in under the 30 day minimum. If can be done by 2 rules
- If a single rule is used to transition S3 objects to S3 Standard-IA or S3 One Zone-IA and then to a glacier class, an additional 30 days waiting period is required before the second transition.
    - The second 30 day waiting period can be avoided by using 2 rules.

## Replication
There are two types of replication:
- **S3 Same-Region Replication (SRR)**
- **S3 Cross-Region Replication (CRR)**

The difference between SRR and CRR is only whether the buckets are in the same AWS account or different AWS accounts.

### Replication Configuration
- Replication is applied to the **source bucket** and specifies which **destination bucket** to replicate to.
- Replication is encrypted over SSL.
- **Requirements**:
    - An **IAM Role** for the replication process must be configured to allow S3 to assume it for the replication process. It must have permission to read objects from the source bucket and write objects to the destination bucket.
    - If the destination bucket is in a different AWS Account, a **bucket policy** (resource policy) must be applied to **trust** the IAM Role so it can write to the destination bucket.
    - Source bucket owner must have permission to read objects from the source bucket. This assumption might be violated if other accounts were given permission to add objects to the source bucket.
    - **Versioning**: Versioning must be enabled on both the source and destination buckets. It can be enabled if it not currently enabled (*exam point*).

### Replication Options
    - **Data Subset**: All objects are replicated by default. A subset of objects can be replicated by specifying a combination of **prefix** or **tags** in the replication configuration.
    - **Storage Class**: The default is to use the same storage class between the source and destination buckets. The destination bucket can be configured to use a different storage class (*exam point*).
    - **Ownership**: The default for the destination bucket is to inherit the ownership of the source bucket if they are in the same AWS Account. If they are in different AWS Accounts, the destination account may not be able to read the objects in the destination bucket; the owner can be overridden in the replication configuration.
    - **Replication Time Control (RTC)**: RTC can be enabled to ensure that objects are replicated with a maximum of 15 minutes of delay (*exam point*). If it is not enabled, the replication process is on a best effort basis.
- **Not Retroactive**: Replication is **NOT** retroactive by default (*exam point*). Only new objects are replicated. AWS recently introduced the ability to replicate existing objects.
- **One-Way Replication**: Replication is **one-way** (*exam point*). Objects are not replicated back to the source bucket. Recently, AWS added bi-directional replication, but it is an additional setting that requires configuration.
- **Encryption**: Objects that are unencrypted, SSE-S3, and SSE-KMS are replicated. Support for SSE-C was recently added.
- **System Events**: Lifecycle system events are not replicated (e.g., deletion marker from a lifecycle event or notification settings from a lifecycle event). 
- **Storage Class**: Glacier and Glacier Deep Archive objects can not be replicated.
- **Delete Marker**: Delete markers are not replicated unless **DeleteMarkerReplication** is enabled in the replication configuration.

### Replication Use Cases

SRR: Log aggregation, syncing TEST and PROD, Resilience with strict sovereignty requirements

CRR: Global resilience and latency reduction


## Presigned URLs
Presigned URLs are temporary URLs that allow users to access objects by **assuming an IAM identity** with access to the object. 

- The presigned URLs access duration can be customized.
- The presigned URL is encoded with the duration and the IAM user's information.
- The presigned URL uses the **just-in-time permissions** of the identity that generated the presigned URL. If permissions are revoked, the presigned URL will no longer work.

Without presigned URLs, an anonymous user would only have access if (1) an IAM identity is created for them (time consuming), (2) provide them AWS credentials (bad security practice), or (3) the bucket is made public (bad security practice).


### IAM User
The person who generates the presigned URL is the IAM user that AWS assumes to access the object on behalf of the anonymous user. Typically, an application IAM user is used to generate the presigned URL.

- An IAM Role is **NOT** recommended because their credentials are **temporary** and may expire before the presigned URL's expiration time (*exam point*).
- It is possible to generate a presigned URL for an object that you do not have access to or for a non-existent object. However, anonymous users would not be able to access the object (*exam point*).

## S3 Select and Glacier Select
S3 can store huge objects (up to 5TB), but frequently only a subset of the data is needed. 

- **S3 Select** and **Glacier Select** support SQL-like queries to **pre-filter** the data and return it. It is faster and cheaper (up to 400% faster and 80% cheaper).
- **Supported Formats**: CSV, JSON, Parquet, BZIP2 for CSV and JSON

## S3 Event Notifications
**S3 Event Notifications** are a feature that delivers notifications to an SNS Topic, SQS Queue, or Lambda function when events occur in an S3 bucket.

- **Supported Events** include but are not limited to: 
    - Object created (PUT, POST, Copy, CompleteMultipartUpload)
    - Object deleted (*, Delete, DeleteMarkerCreated)
    - Object restored (Post(Initiated), Completed)
    - Object replication (OperationMissedThreshold, OperationReplicatedAfterThreshold, OperationNotTracked, OperationFailedReplication)

- Each notification destination (SNS, SQS, Lambda) must have a **resource policy** that allows the **S3 principal** to publish to the destination. Currently, these policies must be implemented via CLI or API.

**EventBridge** is a more modern alternative to S3 Event Notifications and us recommended. It supports more event types and services.

## S3 Access Logs
S3 Access Logs are a feature that logs all requests made to an S3 bucket. It requires a source bucket that is being monitored and a **target bucket** to store the logs.

- Access logging is enabled via the console UI or PUT Bucket Logging via API or CLI.
- Logging is handled by a service called the **S3 Log Delivery Group**. It reads the logging configuration on the source bucket.
- The S3 Log Delivery Group must have write access to the target bucket. This access is granted via an ACL on the target bucket that gives the S3 Log Delivery Group the **WRITE** permission.
- Logs are delivered as log files. Records are newline delimited and attributes are spaced delimited. Logs include the timestamp, requestor, status code, error code, etc.
- A single target bucket can be used by multiple source buckets. Source buckets can be distinguished by prefixing the logs with the source bucket's name in the configuration.
- The lifecycle of the log file must be managed.

## S3 Object Lock
S3 Object Lock is a feature that allows users to prevent objects from being deleted or overwritten. It is a **write-once-read-many** (WORM) model which means that once an object is written, it can **NOT be deleted or **overwritten**.

- Enabled on new S3 buckets. Existing buckets require contacting AWS support.
- Creating a bucket with Object Lock enabled is **irreversible** and **automatically enables versioning**. 
- Individual versions are locked
- There are two modes of object lock:
    - **Retention**: The object is locked for a fixed period of time.
    - **Legal Hold**: The object is locked indefinitely.
    - Both, one, or neither can apply

### S3 Object Lock - Retention
- **Retention Period**: The object version is locked for a time specified in days and years.
    - **Compliance mode** (strictest): Objects can't be adjusted, deleted, or overwritten for the duration of the retention period. 
        - The retention period and mode can **NOT** be changed, **even by the account root user**.
    - **Governance mode**: Objects still can't be adjusted, deleted, or overwritten for the duration of the retention period. 
        - Special permission to allow specific identities to change *lock settings* can be granted using the `s3:BypassGovernanceRetention` permission. 
        - User must provide a header, `x-amz-bypass-governance-retention:true`, to make lock settings changes.
        - Governance mode is useful to prevent accidental deletion of objects, test compliance mode changes, and if there are process or governance reasons to retain versions.

### S3 Object Lock - Legal Hold
- Enabled on an **object version**.
- **No concept of retention period; legal hold is either ON or OFF.**
- No deletes or changes until removed.
- `s3:PutObjectLegalHold` permission is required to add or remove the legal hold status.
- Can be used to prevent accidental deletion or flag an important object version.


## S3 Access Points
S3 Access Points simplifies the management of access to S3 buckets and objects.

Instead of managing 1 bucket with 1 extremely complex bucket policy, you can create **multiple access points** each with a **different policy**. The access points can be thought of as mini-buckets.

- Each access point has its own **endpoint address (DNS Address)**. 
    - These can be given to different teams to get partial access to the bucket.
- Access point can be created either using the console or API.
    - `aws s3control create-access-point --name <access-point-name>  --bucket <bucket-name>` is the CLI command (*exam point - memorize create-access-point*). 
- **Access point policies** control access to the access point. It is functionally **equivalent to a bucket policy**.
    - Access point policies can restrict identities based on prefixes, tags, or actions.
- VPC Resource Access: Access points can also be created to only allow access from a VPC.
    - This approach ties the access point to a specific VPC. 
    - A **VPC endpoint** must be created in the relevant VPC to connect it with the access point. 
    - Access via this route is enforced by endpoint policies.
    - Note: "VPC Origin" is an unrelated Cloudfront feature.
- Any permissions defined on the access point must be defined on the bucket policy.